\documentclass[11pt]{article}
\usepackage{times,latexsym,amsmath,amssymb,amsfonts,bm,mathtools}
\usepackage{array, marginnote, paralist, booktabs, multirow, url, natbib}
\usepackage{graphicx, epstopdf, rotating, color, subfig, adjustbox}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\usepackage[margin=1in]{geometry}
\usepackage[margin=1in]{caption}
\usepackage{relsize}
\usepackage{amsthm}

\SetKwInOut{Parameter}{parameter}
\renewcommand{\familydefault}{ptm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}%
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\mean}{\text{mean}}
\newcommand{\sd}{\text{sd}}
\newcommand{\median}{\text{median}}
\newcommand{\IQR}{\text{IQR}}
\newcommand{\MAD}{\text{MAD}}
\newcommand{\dist}{\text{dist}}
\newcommand{\diag}{\text{diag}}
\newcommand{\nn}{\text{nn}}
\newcommand{\nnd}{\text{nnd}}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcolumntype{d}[1]{D{.}{.}{#1}}  % define "d" column type

\newcommand{\density}{\text{density}}
\newcommand{\AUC}{\text{AUC}}
\newcommand{\margincomment}[1]{\marginpar{\footnotesize{#1}}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}

\graphicspath{{./Graphics/}}

\newcommand{\chk}{$\checkmark$}
\newcolumntype{R}[2]{%
    >{\adjustbox{angle=#1,lap=\width-(#2)}\bgroup}%
    l%
    <{\egroup}%
}
\newcommand*\rot{\multicolumn{1}{R{90}{1em}}}% no optional argument here, please!

% =======================================================================
\begin{document}
% =======================================================================
\title{DOBIN: A basis for outlier detection}
\author{Sevvandi Kandanaarachchi, Rob J. Hyndman}
\maketitle
\abstract{FOR LATER}

% =======================================================================
\section{Introduction}
% =======================================================================
FOR LATER!

% =======================================================================
\section{Related work}
% =======================================================================
FOR LATER!

% =======================================================================
\section{Mathematical Framework}\label{sec:MathFrame}
% =======================================================================
Let $X_{N \times p}$ be a matrix denoting a dataset of $N$ observations and $p$ attributes. Let us denote the  $i^{\text{th}}$ observation in $X$ by $\bm{x}_i$. The  distance between points $\bm{x}_i$ and $\bm{x}_j$  can be written as 
\begin{equation}\label{eq:secMF1}
\dist(\bm{x}_i, \bm{x}_j)^2 = \left( \bm{x}_i - \bm{x}_j \right)^T S \left( \bm{x}_i - \bm{x}_j \right) \, , 
\end{equation}
where $S$ is a symmetric positive definite matrix.  In addition when $S$ is diagonal, i,e, $S = \diag(s_1, s_2, \ldots s_p)$ we obtain
\begin{equation}\label{eq:secMF2}
    \dist(\bm{x}_i, \bm{x}_j)^2 = \left\langle \eta\, ,  \left( \bm{x}_i - \bm{x}_j \right)^2 \right\rangle\, 
\end{equation}
where $\left\langle \cdot\, , \cdot \right\rangle$ denotes the standard inner product in $\mathbb{R}^p$,   $\eta = \left(s_1, s_2, \ldots s_p\right)^T$, and with some abuse of notation $\left( \bm{x}_i - \bm{x}_j \right)^2$ denotes the element wise difference of $\left( \bm{x}_i - \bm{x}_j \right)$ squared. As $S$ is symmetric and positive definite  it is diagonalizable, thus giving us motivation for considering a diagonal $S$. 

\subsection{The $Y$ space}\label{sec:MathFrame1}
Let  $\bm{y}_{ij} = \left( \left( x_{i1} - x_{j1} \right)^2, \left( x_{i2} - x_{j2} \right)^2, \ldots,  \,  \left( x_{ip} - x_{jp} \right)^2  \right)\, . $ Substituting in equation~\eqref{eq:secMF2} gives
\begin{equation}\label{eq:secMF3}
    \dist(\bm{x}_i, \bm{x}_j)^2 = \left\langle \eta\, ,  \bm{y}_{ij} \right\rangle\, \, .
\end{equation}
As such, by finding the appropriate $\eta$ of unit length,  we can maximize the distance between $\bm{x}_i$ and $\bm{x}_j$.  This brings us to the question of which $\bm{x}_i$ and $\bm{x}_j$ needs to be chosen to maximize the distance between them. If we use knn distances as a guiding principle, we are interested in the $k$ nearest neighbours of a given point. Consequently, we can choose $\bm{x}_i$ and $\bm{x}_j$ that share a common neighbourhood. Also we note that if we were to calculate $\bm{y}_{ij}$ for all pairs, it would be computationally expensive as there are $N(N+1)/2$ pairs. In addition, this would give us pairwise distances between points that we are not interested in, such as points on the boundary of the point cloud, that are opposite each other. Finding  $\eta$ that maximizes the distances between such points is not conducive to outlier detection, because a large distance between such polar-opposite points does not mean that either of those points are outliers. As such, we select the  $\bm{x}_i$ and $\bm{x}_j$ pairs that we want to include in our $Y$ space. \\

For each point $\bm{x}_i$ we compute $\bm{y}_{ij}$ arising from a set a nearest neighbours. From \cite{kandanaarachchi2018normalization} we know that nearest neighbours depend on the normalization technique. As a result, we choose two normalization techniques for pre-processing the data; Min-Max and Median-IQR. Min-Max normalization scales each column to values between $0$ and $1$, with the minimum mapped to $0$ and the maximum mapped to $1$.  Median-IQR scales each column to median $0$ and IQR $1$. Let $X_1$ denote the normalized data using  Min-Max and $X_2$ using Median-IQR. For each normalization technique we choose a set of nearest neighbours from $k_1$ to $k_2$, that is for any point we choose the $k_1^{\text{th}}$ nearest neighbour to  $k_2^{\text{th}}$ nearest neighbour. Here $k_1$ and $k_2$ are parameters and one can choose $k_1 =0$. \\

Next for each point we find the union of nearest neighbours computed using the two normalization techniques. For example for a given point $\bm{x}_i$ if Min-Max gave rise to the nearest neighbours $\{\bm{x}_2, \bm{x}_5, \bm{x}_9 \}$ and Median-IQR   $\{\bm{x}_5, \bm{x}_4, \bm{x}_7 \}$, then we consider the union of these two sets as the set of nearest neighbours of $\bm{x}_i$. Using the nearest neighbours we construct a list of pairs denoted by $T$. Continuing the same example the pairs $(\bm{x}_i, \bm{x}_2), \,  (\bm{x}_i, \bm{x}_5), \, (\bm{x}_i, \bm{x}_4), \, \ldots $ and so on will be in $T$. \\

For each pair in $T$, we compute $\bm{y}_{ij}$ as in equation~\eqref{eq:secMF2} on the normalized space $X_1$ or $X_2$, depending on user preference. Now we have the initial $Y$ space; a set of $M$ points in $\mathbb{R}^{p+}$. As the pairs of points that give rise to the initial $Y$ space are contained in $T$, we can refer to $Y = \{ \bm{y}_l \}_{l=1}^M$, where each $\bm{y}_l = \bm{y}_{i_{(l)}, j_{(l)} }$, i.e. $l$ is the row number in the matrix $Y$ and each $\bm{y}_l$ comes from the $l^{\text{th}}$ pair in $T$. \\

We are interested in neighbouring distances that are relatively large. The Euclidean distance squared between two points in the $X_1$ or $X_2$ space  can be written as $\sum_{m=1}^p y_{lm}$ for the appropriate $l$. In order to consider only points with relatively large distances, we remove points $\bm{y}_l$ that contribute to distances below a certain percentile, i.e. we remove points $\bm{y}_l$ such that  $\sum_{m=1}^p y_{lm} < Q$ where $Q$ is the $q^{\text{th}}$ percentile of $ \left \{\sum_{m=1}^p y_{lm} \right \}_{l=1}^M$. We remove the associated pairs from $T$ as well. This constitutes the $Y$ space. \\

The main advantage of the $Y$ space is that it makes distance computation  between two points in the $X$ space a linear function. This makes maximizing distances between points a much easier task. 
Algorithm~\ref{algo:YSpace} summarizes the construction of the $Y$ space. 

%The user-defined parameters for the construction of the $Y$ space are $k_1, k_2$ and $q$, with default values of 

\DontPrintSemicolon
\begin{algorithm}\fontsize{9}{10}\selectfont
	\SetKwInOut{Input}{input~~~}
	\SetKwInOut{Output}{output}
	\Input{~$X$, $k_1, k_2 \in \mathbb{Z}^+$ and $q \in (0, 1)$.}
	\Output{~The space $Y$ consisting of all $y_{ij}$ pairs and the associated indices $i$ and $j$.}
	Normalize $X$ using Min-Max normalization. Let us call this space $X_1$.  \\
	Find $k_1$ to $k_2$ nearest neighbours for each point in  $X_1$. \\
	Next normalize $X$ using Median-IQR normalization. Let us call this space $X_2$.  \\
	Find $k_1$ to $k_2$ nearest neighbours for each point in  $X_2$. \\
	Then for each point collate all unique neighbours using both Min-Max and Median-IQR normalization methods.   \\
	Let $T = $ the set of $i$ and $j$ indices, corresponding to the pairs of points in $X$. \\
	For all these pairs, compute $\bm{y}_{ij}$ as in equation~\eqref{eq:secMF3} using $X_1$ space. Let $Y =\left\{ \bm{y}_{1} =\bm{y}_{i_{(1)},j_{(1)}} , \bm{y}_{2} =\bm{y}_{i_{(2)},j_{(2)}}, \ldots, \bm{y}_{M} =\bm{y}_{i_{(M)},j_{(M)}} \right\}$, where $\left( i_{(1)},j_{(1)} \right)$ is the first $(i, j)$ pair. Consequently $Y$ is an $M\times p$ matrix, with the $l^{\text{th}}$ row denoted by $\bm{y}_l$.\\
	Find the $q^{\text{th}}$ percentile of $ \left\{\sum_{m=1}^p \bm{y}_{lm}\right\}$. The quantity $\sum_{m=1}^p \bm{y}_{lm}$ is the distance between points $\bm{x}_{i_{(l)}}$ and $\bm{x}_{j_{(l)}}$ in  $X_1$  space. \\
	Remove points $\bm{y}_l$ which are less than the  $q^{\text{th}}$ percentile of  $ \sum_{m=1}^p \bm{y}_{lm} $. \\
	Remove the associated pairs of $(i, j)$ from $T$.
	\caption{\itshape Construction of the $Y$ space}
	\label{algo:YSpace}
\end{algorithm}

\begin{figure}[!t]
	\centering
	\subfloat[][]{\includegraphics[scale=0.5]{X_space.pdf}\label{fig:XandY1}}
	\subfloat[][]{\includegraphics[scale=0.5]{Y_space.pdf}\label{fig:XandY2}}
	\caption{The $X$ and $Y$ spaces. The $X$ space in (a) and the associated $Y$ space in (b) with $q = 0.95$. The red coloured point in the $X$ space gives rise to 6   points, which are coloured in red in the $Y$ space.}
	\label{fig:XandY}
\end{figure} 

%% Explanation of fig:XandY
Figure~\ref{fig:XandY} illustrates  the $X$ and $Y$ spaces for which the $X$ space is a 2 dimensional normal distribution of 100 points with a single outlier depicted in red. We see that the $Y$ space consists of a lesser number of points, of which 6 points are contributed by the outlier. This is because $k_1$ to $k_2$ neighbours are considered in the construction of the $Y$ space and the  points $\bm{y}_k$ with relatively small distances $\left(\sum_i y_{ki} \leq Q\right)$ are removed. Thus, the effect of outliers are magnified in the $Y$ space. \\

Before turning to maximising distances, we summarise  key features of the $Y$ space below:
\begin{enumerate}
    \item $Y = \{\bm{y}_l \}_{l=1}^M$, where $\bm{y}_l \in \mathbb{R}^{p+}$.
    \item Each $\bm{y}_l \in Y$ is constructed from two points $\bm{x}_i$ and $\bm{x}_j$ in $X$ with $T(l) = (i,j)$, where $T$ is a matrix consisting of the list of pairs in $X$ that are used to construct $Y$. In addition $\bm{x}_i$ and $\bm{x}_j$ are $k$-nearest neighbours for some $k \in \{k_1, \ldots, k_2\}$.
    \item For $i,j,l$ as in above,  $\bm{y}_l = \left(\bm{x}_i - \bm{x}_j \right)^2$, where with some abuse of notation $\left(\bm{x}_i - \bm{x}_j \right)^2$ denotes the element-wise difference squares. 
    \item The $Y$ space contains points $\bm{y}_l$ with relatively high $\sum_{m=1}^p y_{lm}$.  
\end{enumerate}

\subsection{Maximising the distance between points}\label{sec:MathFrame2}
The $Y$ space has points $\bm{y}_l$ that correspond to neighbouring points $\bm{x}_i$ and $\bm{x}_j$ in $X$, which have relatively high $k$ nearest neighbour distances. From equation~\eqref{eq:secMF3} we obtain
\begin{equation} \label{eq:secMF4}
    \sum_{(i,j) \in T}\dist(\bm{x}_i, \bm{x}_j)^2 = \sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle\, \, .
\end{equation}
Thus  the total distance squared for relatively high nearest neighbour distances $ \sum_{(i,j) \in T}\dist(\bm{x}_i, \bm{x}_j)^2$  can be maximised by finding the appropriate $\eta$ which maximises $\sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle$. However, by choosing $\eta_1 = c \eta_0 $, with $c >1$ will automatically increase the value of $\sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle$ by $c$. Thus, we restrict $\eta$ such that $\left \lVert \eta \right\rVert = 1$.  We now state our optimisation problem as
\begin{align} \label{eq:secMF5}
     \max_\eta & \sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle \, , \\ 
    \left \lVert \eta \right\rVert^2  & = 1 \notag \, .
\end{align}
Solving this using Lagrange Multipliers we obtain $\nabla f  = \lambda \nabla g$, with 
\begin{align}\label{eq:secMF6}
    f(\eta) & = \sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle  \, , \text{and} \\
    g(\eta)  & = \left \lVert \eta \right\rVert^2  - 1 = 0 \, . \notag
\end{align}
As $\nabla f = \left( \frac{\partial f}{\partial \eta_1},  \frac{\partial f}{\partial \eta_2},  \cdots, \frac{\partial f}{\partial \eta_p} \right)$ we compute
\begin{align}\label{eq:secMF7}
\frac{\partial f}{\partial \eta_j} & =  \frac{\partial }{\partial \eta_j}\sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle \, ,  \notag  \\
& = \frac{\partial }{\partial \eta_j} \sum_{l=1}^M \sum_{i=1}^p \eta_i y_{li} \, ,\notag \\
& = \sum_{l=1}^M \sum_{i=1}^p  \delta_{ij} y_{li} \, , \notag \\
& = \sum_{l=1}^M y_{lj} \, . 
\end{align}
where $\delta_{ij}$ is the Kronecker delta function which equals 1 when $i=j$ and zero otherwise. Thus
\begin{equation}\label{eq:secMF8}
    \nabla f = \left(\sum_{l=1}^M y_{l1}, \sum_{l=1}^M y_{l2}, \ldots, \sum_{l=1}^M y_{lp}   \right) = \sum_{l=1}^M \bm{y}_l \, .
\end{equation}
Similarly as $g(\eta) = \left( \sum_{j=1}^p \eta_j^2\right) - 1$  we get $\nabla g = \eta$. By substituting  $\nabla f = \lambda \nabla g$ and using $\left\lVert \eta \right\rVert = 1 $ we obtain
\begin{equation}\label{eq:secMF9}
    \eta = \frac{\sum_{l=1}^M \bm{y}_l }{  \left\lVert\sum_{l=1}^M \bm{y}_l  \right\rVert  }
\end{equation}
Thus, we have computed $\eta$ that maximises distance squared in $Y$.  

\subsection{Constructing a basis}\label{sec:MathFrame3}
We have computed the vector $\eta$ that maximises $\sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle$. To find a basis,  we need to find the second, third and $i^{\text{th}}$ vectors which are independent and in some way maximise the quantity $\sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle$. Let us first rename $\eta$ as $\eta_1$. Taking $\eta_1$ as the first basis vector, we take the projection of vector $\bm{y}_l$ on to $\eta_1$  and remove these components from $\bm{y}_l$. 
\begin{equation}\label{eq:secMF10}
    \bm{y}_{l_1} = \bm{y}_l - \left\langle \eta_1 \, , \bm{y}_l \right \rangle \eta_1
\end{equation}
Thus $\bm{y}_{l_1} \perp \eta_1$. Let $Y_1 = \{\bm{y}_{l_1} \}_{l=1}^M $ be the set of remaining components of $\{ \bm{y}_l \}_{l=1}^M$ after removing the projection on to $\eta_1$. Now we can compute $\eta_2$ which maximises $\sum_{l}\left\langle \eta\, ,  \bm{y}_{l_1} \right\rangle$ as
\begin{equation}\label{eq:secMF11}
    \eta_2 = \frac{\sum_{l=1}^M \bm{y}_{l_1} }{  \left\lVert\sum_{l=1}^M \bm{y}_{l_1}  \right\rVert  } 
\end{equation}
Proceeding in this way we obtain 
\begin{align}
    \bm{y}_{l_{b}} & = \bm{y}_{l_{b-1}} - \left\langle \eta_b \, , \bm{y}_{l_{b-1}} \right \rangle \eta_b \, , \label{eq:secMF12} \\
     \eta_{b+1} & = \frac{\sum_{l=1}^M \bm{y}_{l_b} }{  \left\lVert\sum_{l=1}^M \bm{y}_{l_b}  \right\rVert  } \label{eq:secMF13}
\end{align}
with $\bm{y}_{l_0} = \bm{y}_{l}$. The set $\Theta = \left\{ \eta_1, \eta_2, \ldots , \eta_p  \right\}$ gives a basis for $Y$ which aims to maximise the quantity $\sum_{l}\left\langle \eta\, ,  \bm{y}_{l} \right\rangle$.  \\

Next we draw attention to a technical point. Equation~\ref{eq:secMF4} is only true for  $\bm{y}_l \in \mathbb{R}^{p+}$. The other $Y$ spaces $Y_1, Y_2, \ldots $ are not  subsets of $\mathbb{R}^{p+}$. To adjust for this, after computing  $\bm{y}_{l_{b}}$ in equation~\eqref{eq:secMF10} we temporarily change the basis such that  $\{ \bm{y}_{l_{b}}\}_{l=1}^M$ are in a positive orthant.  Then after computing  $\eta_{b+1}$ in equation~\eqref{eq:secMF11} using positive $\{ \bm{y}_{l_{b}}\}_{l=1}^M$,  we transform it back again to the original basis.  \\

Once the basis $\Theta$  is computed, we  change the basis of the original data as follows:
\begin{equation}\label{eq:secMF14}
    \tilde{X} = X_1 \Theta \, .
\end{equation}
We note that the basis $\Theta$ is not invariant to rotations, i.e. if one were to start with a different basis, the resulting $\Theta$ would be different. This is because when constructing the $Y$ space we compute element-wise squares of vector differences. While vector addition is basis invariant, squared element-wise components depend on the basis vectors. Notwithstanding this limitation, we achieve good results on experiments conducted with our extensive collection of more than $12,000$ real world datasets. We performed DOBIN using the original basis on almost all our experiments. We only used a different basis if the number of variables  were much higher than the number of observations.  DOBIN generally performs better with the original basis than with a transformed basis, and finds it  is easier to compute the true direction of outliers when the axes are independent, compared to when each axis is a linear combination of the original variables. 


% GIVE NAMES TO THE BASIS VECTORS, AND COORDINATES
% =======================================================================
\section{Experiments with synthetic data}\label{sec:synthetic}
% =======================================================================
Next we do three experiments with synthetic data. For each experiment we compare results for $3$ outlier detection methods, namely LOF \citep{breunig2000lof}, KNN \citep{ramaswamy2000efficient} and isolation forest \citep{liu2008isolation}. We choose these methods as they are fundamentally different: LOF uses a local density based approach; KNN uses a global distance based approach; and iForest uses a  tree based approach.  For each outlier detection method we consider the following $3$ coordinate systems:  
\begin{enumerate}  
\item All variables in the dataset
\item Perform Principal Component Analysis (PCA) on the dataset, and use the first half of the principle components (PCs).
\item Perform DOBIN on the dataset, and use the first half of the DOBIN components. 
\end{enumerate}
For each outlier detection method, we compare the results using these $3$ sets of coordinates. We use area under the Receiver Operator Characteristic curve (AUC) as our performance measure. 


\subsection{Experiment 1}
The first experiment considers two normal distributions: an  inlier normal distribution and an outlier normal distribution, which moves out from the inlier distribution as its mean increases. We consider a dataset of $405$ observations in $\mathbb{R}^6$, of which $400$ observations in each dimension are normally distributed with mean $0$ and standard deviation $1$. The remaining $5$ observations signify outliers and are normally distributed with mean $\mu$ and standard deviation $0.2$ in one dimension, and mean $0$ and standard deviation $1$ in other dimensions. The value of $\mu$ is changed from $0$ to $4.5$ by increments of $0.5$. The reason for a smaller standard deviation in the first dimension is to ensure that the outliers move out of the normal distribution. For each value of $\mu$ we consider the three sets of coordinates described above and their performance using the outlier methods LOF, KNN and iForest.  We conduct this experiment for $10$ iterations. Figure~\ref{fig:Exp1} shows the results for all $3$  outlier methods using the average values of  10 iterations. 

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.8]{Exp1.pdf}
	\caption{Results of Experiment 1, in $\mathbb{R}^6$.}
	\label{fig:Exp1}
\end{figure} 

Using $3$ DOBIN components gives the highest performance for all $3$ outlier detection methods for $\mu > 1$.  Indeed, we are interested in a set of coordinates can accentuate the the outliers as they move out from the inlier distribution.  For values of $\mu \in \{0, 0.5, 1\}$,  the outlier distribution lies in the interior of the inlier distribution, making a performance comparison between the coordinates not meaningful.   

\subsection{Experiment 2}
The second experiment considers a bi-modal setting with two inlier normal distributions and one outlier normal  distribution which moves into the valley as its mean changes. Somewhat similar to the previous example we consider $805$ observations in $\mathbb{R}^6$; $800$ inlier observations of which $400$  centred at $(5,0,0,0,0,0)$ and the other $400$ centred at $(-5,0,0,0,0,0)$. All inlier observations come from normal distributions with standard deviation $1$ in each dimension. The outlier distribution consists of $5$ points with mean $(5-\mu,0,0,0,0,0)$ and standard deviations $0.2$ in the first dimension and $1$ in other dimensions. Again, a smaller standard deviation is to ensure that the outliers move into the valley. Figure~\ref{fig:Exp2} shows the results for all $3$  outlier methods on all $3$ coordinate systems using the average values of 10 iterations. \\

Again we see that using 3 DOBIN components gives the highest performance for all 3 outlier methods. In particular, 
\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.8]{Exp2.pdf}
	\caption{Results of Experiment 2, in $\mathbb{R}^6$.}
	\label{fig:Exp2}
\end{figure} 

\subsection{Experiment 3}
The third experiment is motivated from \cite{zimek2012survey}. We consider a uniform distribution in $\mathbb{R}^{20}$ and place a manual outlier at $0.9$ in $i$ dimensions where $i$ is changed from $1$ to $20$. While this is not an outlier in any single dimension, as $i$ increases this point stands out from the rest of the distribution. Indeed, for large $i$ this  point lies furthest from the rest. An equivalent outlier is $0.1$ in $i$ dimensions. The reason these points are outliers is because they are at the corners of the hyper-cube and as such are far away from other points. On the contrary, $0.5$ in $i$ dimensions would be closest to other points. This is because $(0.5, 0.5, \ldots)$ has neighbours from all sides and has on average $2^{20}$ more neighbours compared to $(0.9, 0.9, \ldots)$, which is at a corner on the unit hyper-cube.  

Figure~\ref{fig:Exp3} shows the results of all three outlier methods on all $3$ coordinate systems. Again we see that using half of the DOBIN components gives the best performance for all three outlier methods.    \\

In all three experiments we see that DOBIN coordinates give the best performance followed by the full set of coordinates. Principal Components are generally less effective than the full set of coordinates for these $3$ outlier methods. We note that this is not a failure of PCA, as the purpose of PCA is not outlier detection. Rather, PCA maximizes the variance in each component such that a low dimensional representation of the dataset is a good approximation to the original. However, PCA is used in outlier detection when dimension reduction is preferred before applying outlier detection methods \citep{talagala2019anomaly, hyndman2015large}. For this reason we compare DOBIN with PC components, and propose DOBIN as an alternative to PCA as a dimension reduction technique for outlier detection.



\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.8]{Exp3.pdf}
	\caption{Results of Experiment 3, in $\mathbb{R}^{20}$.}
	\label{fig:Exp3}
\end{figure} 

% REDO EXPERIMENT 1 AND 2 WITH REFLE = FALSE 
% =======================================================================
\section{Results with visualization}\label{sec:ResWithVis}
% =======================================================================
In this section we investigate specific examples motivated from \cite{unwin2019multivariate} and \cite{wilkinson2017visualizing} using the O3 plot. O3 plots \citep{unwin2019multivariate} can be used to compare the results of $6$ different outlier detection methods, namely \textit{HDoutliers} \citep{wilkinson2017visualizing}, \textit{mvBACON} \citep{billor2000bacon},  \textit{adjOutlyingness} \citep{brys2005robustification}, \textit{covMcd} \citep{rousseeuw1999fast} , \textit{FastPCs} \citep{vakili2014finding} and \textit{DetectDeviatingCells} \citep{rousseeuw2018detecting}. As such, O3 plots act as an ensemble method. In addition, O3 plots also highlight outliers in axis parallel subspaces. 

\subsection{\textit{Election2005} dataset}\label{sec:ResWithVis1}
We start with the \textit{Election2005} dataset \citep{mbgraphic}, which is used by \cite{unwin2019multivariate}  to illustrate the O3 plot. This dataset includes election results of two German elections and certain attributes of the electorates. Here we repeat one of the examples conducted by  \cite{unwin2019multivariate} and examine the associated  first two DOBIN components (DC1-DC2). \\

\begin{figure*}[!t]
	\centering
	\subfloat[][]{
		\includegraphics[scale=0.5]{Ex1_1.pdf}
		\label{fig:Election20051}
	}%
	\subfloat[][]{
		\includegraphics[scale=0.5]{Ex1_2.pdf}
		\label{fig:Election20052}
	}%
	\caption{O3plot of the \textit{Election2005} dataset in Figure~\ref{fig:Election20051} and the first 2 DOBIN components in Figure~\ref{fig:Election20052} }
	\label{fig:Election2005}
\end{figure*}

Figure~\ref{fig:Election20051} shows the O3 plot of this dataset using all $6$ outlier methods. The columns in the left indicate the variables, the columns in the right indicate the observations, the rows show the axis parallel subspaces and the colours depict the number of methods that identify each observation in each subspace as an outlier. From this plot we see that observation $X84$ is identified as an outlier by $5$ methods in $4$ subspaces, $4$ methods in $2$ subspaces, $3$ methods in $1$ subspace and $2$ methods in $1$ subspace. $X84$ is arguably the most outlying observation in this dataset. The observations $X83$, $X76$, $X82$ are also identified as outliers by $5$ methods in the dimension of population density. They are also identified as outliers by multiple methods in different subspaces. The layout of the O3 plot is such that the outlyingness of the observations increase to the right. \\

Figure~\ref{fig:Election20052} shows the first $2$ DOBIN components of the \textit{Election2005} dataset. This is a projection of the dataset onto a two dimensional subspace spanned by the first $2$ DOBIN vectors. Here we see the observation $X84$ far away from the rest of the data with $X76$, $X83$ and $X82$ somewhat detached from the rest. The observations $X87$ an $X81$ are almost at the same position in this $2$-dimensional space. The observations $X221$ and $X21$ also appear a bit outside the boundary of  other points. A simple KNN algorithm in the DC1-DC2 space gives the top $8$ candidates as $84$, $76$,  $83$, $82$, $221$, $81$, $87$, and  $21$. Thus, the DC1-DC2 space accentuates the eight most outlying observations according to the O3 plot, while showing clearly $X84$ is the most outlying observation. \\

The first DOBIN vector is of interest to us as the outliers deviate in this direction. Equation~\eqref{eq:ResWithVis1} gives DC1 as a linear combination of the input variables. 

\begin{equation}\label{eq:ResWithVis1}
    \text{DC1} = \begin{bmatrix}
    0.008 & 0.998 & 0.040 & -0.050  
    \end{bmatrix}
    \begin{bmatrix}
    \text{Area} \\
    \text{Population Density} \\
    \text{Birthrate} \\
    \text{Car Ownership}
    \end{bmatrix}
\end{equation}
From equation~\eqref{eq:ResWithVis1} we see that \textit{Population Density} is the main variable contributing to outliers in the DC1-DC2 space. The O3 plot supports this observation. If we look carefully at the O3 plot we see that $X21$ to $X84$ gets identified as outliers in  all subspaces that contain \textit{Population Density}.  This insight gives us a better understanding of the dataset. 

\subsection{\textit{Diamonds} dataset}\label{sec:ResWithVis2}
The next example is taken from the \textit{Diamonds} dataset \citep{ggplot2} which is included in the \textit{O3Outliers} R package \citep{O3Rpack}. This dataset contains attributes of diamonds. For this example we compare the O3 plot with the associated DC1-DC2 space.  \\ 

\begin{figure*}[!t]
	\centering
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex2_1.pdf}
		\label{fig:Diamonds1}
	}%
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex2_2.pdf}
		\label{fig:Diamonds2}
	}%
	\caption{O3plot of the \textit{Diamonds} dataset in Figure~\ref{fig:Election20051} and the first 2 DOBIN components in Figure~\ref{fig:Election20052} }
	\label{fig:Diamonds}
\end{figure*}

Figure~\ref{fig:Diamonds1} shows the O3 plot of this dataset using $3$ outlier methods, namely \textit{HDoutliers}, \textit{FastPCs} and \textit{adjOutlyingness}. We could not use all $6$ methods on this dataset as other outlier methods included in the \textit{OutliersO3} R package gave computational errors. Of the $3$ methods used, all methods identified $X4792$, $X2315$ and $X2208$ as outliers in 6 subspaces including the full space. These $3$ points appear in the DC1-DC2 space in the bottom right corner. In addition, the point $4519$ appear in the top right corner in the DC1-DC2 space, away from other points. Comparing with the O3 plot, we see that $X4519$ is identified as an outlier by $2$ methods in $6$ subspaces. In keeping with the guidelines of the previous section, we use half of the DC components to evaluate a KNN algorithm. The top $4$ candidates in this $3$-dimensional DC space are $4792$, $2315$, $2208$ and $4519$ in that order. \\

To understand the first DOBIN vector, we look at its coefficients:
\begin{equation}\label{eq:ResWithVis2}
    \text{DC1} = \begin{bmatrix}
    0.098 & -0.659 &  0.185 & 0.097  & 0.126 & -0.704  
    \end{bmatrix}
    \begin{bmatrix}
    \text{Carat} \\
    \text{Depth} \\
    \text{Table} \\
    \text{x} \\
    \text{y} \\
    \text{z}
    \end{bmatrix}
\end{equation}
We see that the variables \textit{z} and \textit{Depth} contribute more  to  outliers in the DC1-DC2 space as well as in the O3 plot. 
%Thus the DOBIN basis can be used to project data for easy visualization of outliers. 


\subsection{\textit{USArrests} dataset}\label{sec:ResWithVis3}
The popular \textit{USArrests} dataset  is studied by \cite{bailey1995interactive, sarkar2008labels, yaminiviolent} to name a few. This dataset contains violent crime rates for the 50 US states in 1973. The O3 plot using all $6$ outlier detection methods and the DC1-DC2 space of this dataset is illustrated in Figure~\ref{fig:USArrests}. 

\begin{figure*}[!t]
	\centering
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex3_1.pdf}
		\label{fig:USArrests1}
	}%
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex3_2.pdf}
		\label{fig:USArrests2}
	}%
	\caption{O3plot of the \textit{USArrests} dataset in Figure~\ref{fig:USArrests1} and the first 2 DOBIN components in Figure~\ref{fig:USArrests2} }
	\label{fig:USArrests}
\end{figure*}

From Figure\ref{fig:USArrests1} we see that $X2$ is flagged as an outlier in $8$ subspaces including the full space, by one or two outlier detection methods. The adjoining outlier $X33$  is only flagged in $4$ subspaces and the others in even smaller subspaces.  The observation $X2$ corresponds to Alaska. Of the $6$ outlier methods, only $2$ identify Alaska as an outlier in certain subspaces. This also suggests that even though there is consensus between two methods, as only a third of the methods agree on this outlier, it may not be quite so outlying as the outliers in the previous examples. Indeed, this notion is validated by the DC1-DC2 plot. We see the observation $2$ standing out from the rest, but not by a massive margin. A KNN algorithm identifies this observation as having the highest KNN distance in DC1-DC2 space.   \\   

We look at the coefficients of the first DOBIN vector as observation $2$ deviates in DC1:
\begin{equation}\label{eq:ResWithVis3}
    \text{DC1} = \begin{bmatrix}
    0.173 & 0.077 &  -0.675 & 0.713   
    \end{bmatrix}
    \begin{bmatrix}
    \text{Murder} \\
    \text{Assault} \\
    \text{UrbanPop} \\
    \text{Rape} 
    \end{bmatrix} \, 
\end{equation}
We see that the variables \textit{UrbanPop} and \textit{Rape} contribute more to the first DOBIN vector than the other variables. From equation~\eqref{eq:ResWithVis3},  high values of DC1 indicate  low percentages of urban population and high rape arrests per $100,000$ residents. As such, we might expect Alaska to have higher rape arrests per urban population percentage. Indeed from Figure~\ref{fig:USArrests3} we see that Alaska has the highest ratio of  \textit{Rape/UrbanPop} of all $50$ states confirming the insight gleaned from the first DOBIN vector. 

\begin{figure*}[!t]
	\centering
	\includegraphics[scale=0.48]{Ex3_3.pdf}
	\caption{USArrests dataset -- Rape to Urban population ratio values}
	\label{fig:USArrests3}
\end{figure*}


\subsection{\textit{Airquality} dataset}\label{sec:ResWithVis4}
The \textit{Airquality} dataset discussed in \cite{john1983graphical} has measurements on air quality in New York city from May to September 1973.  The O3 plot using all $6$ outlier methods and the DC1-DC2 space of this dataset is shown in Figure~\ref{fig:Airquality}.

\begin{figure*}[!t]
	\centering
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex4_1.pdf}
		\label{fig:Airquality1}
	}%
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex4_2.pdf}
		\label{fig:Airquality2}
	}%
	\caption{O3plot of the \textit{Airquality} dataset in Figure~\ref{fig:Airquality1} and the first 2 DOBIN components in Figure~\ref{fig:Airquality2} }
	\label{fig:Airquality}
\end{figure*}
The O3 plot in Figure~\ref{fig:Airquality1} shows $X117$ as the only outlier identified in $6$ subspaces by $2$ methods. The associated DC1-DC2 space shows the observation $117$ away from the rest at the bottom right. Again, we see agreement between the O3 plot and DC1-DC2 space. The observation $X117$ was identified as an outlier only by a third of the outlier methods. This is corroborated in the DC1-DC2 plot by comparatively less outlyingness of observation $117$ compared to  outliers in the \textit{Diamonds} dataset. Performing a KNN algorithm revealed the observation with the highest KNN distance as $117$ in the DC1-DC2 space. \\

The first DOBIN vector has the following coefficients:
\begin{equation}\label{eq:ResWithVis4}
    \text{DC1} = \begin{bmatrix}
    0.842 & -0.078 &  0.512 & -0.151   
    \end{bmatrix}
    \begin{bmatrix}
    \text{Ozone} \\
    \text{Solar.R} \\
    \text{Wind} \\
    \text{Temp} 
    \end{bmatrix} \, 
\end{equation}
From equation~\eqref{eq:ResWithVis4} we see that the observation $117$ is an outlier due to high values of \textit{Ozone} and \textit{Wind}. 

\subsection{\textit{Lesmis} dataset}\label{sec:ResWithVis5}
The \textit{Lesmis} dataset \citep{sombrero} contains the character coappearance network of characters in the novel \textit{Les Miserables} by Victor Hugo. This dataset is stored as a graph, with vertices corresponding to characters and is shown in Figure~\ref{fig:lesmisgraph}. The character network in \textit{Les Miserables} is also studied in \cite{wilkinson2017visualizing} with the focus of finding outliers. Here we conduct a similar study using the O3 plot and the DC1-DC2 space. \\

\begin{figure*}[!t]
	\centering
	\includegraphics{lesmis.pdf}
	\caption{The \textit{lesmis} dataset }
	\label{fig:lesmisgraph}
\end{figure*}

As in \cite{wilkinson2017visualizing} we compute the graph-based features centrality, transitivity, closeness, betweenness, degree, average nearest neighbour degree and page rank for each character in the dataset. This transforms the original graph to a rectangular dataset of $77$ observations and $7$ variables, with each observation corresponding to a character in the novel.  Figure~\ref{fig:lesmis1} shows the O3 plot for the transformed \textit{lesmis} dataset using the $3$ methods \textit{HDoutliers}, \textit{mvBACON} and \textit{covMCD}. The other $3$ outlier methods used in O3 plots gave computational errors and as such could not be used.  Figure~\ref{fig:lesmis2} shows the DC1-DC2 space of the transformed \textit{lesmis} dataset.  \\

\begin{figure*}[!t]
	\centering
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex5_1.pdf}
		\label{fig:lesmis1}
	}%
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex5_2.pdf}
		\label{fig:lesmis2}
	}%
	\caption{O3plot of the transformed \textit{lesmis} dataset in Figure~\ref{fig:lesmis1} and the first 2 DOBIN components in Figure~\ref{fig:lesmis2} }
	\label{fig:lesmis}
\end{figure*}

From the O3 plot it is evident that Valjean is the main outlier consistently identified by the $3$ outlier methods in a vast majority of the subspaces.  This is confirmed by the DC1-DC2 plot as Valjean appears far away from the rest of the characters in the top right corner.  The next most outlying characters in the O3 plot are Myriel, Gavroche, Marius and Fantine. These characters also appear in the DC1-DC2 space somewhat detached from the rest of the characters, although none so remarkably far away as Valjean. A KNN algorithm performed on the first half of the DC components identified Valjean, Myriel, Gavroche, Marius and Fantine as having the highest KNN distances. 

As the first DOBIN vector is discriminatory, we look at its coefficients:
The first DOBIN vector has the following coefficients:
\begin{equation}\label{eq:ResWithVis5}
    \text{DC1} = \begin{bmatrix}
    0.0022 & -0.0001 &  0.0032 & 0.9999 & 0.0029 & 0.0025 & 0.0057  
    \end{bmatrix}
    \begin{bmatrix}
    \text{Centrality} \\
    \text{Transitivity} \\
    \text{Closeness} \\
    \text{Betweenness} \\
    \text{Degree} \\
    \text{Avg. Neighbour Degree}\\
    \text{Page Rank}
    \end{bmatrix} \, .
\end{equation}
From equation~\eqref{eq:ResWithVis5} we see that \textit{Betweenness} is the main feature that makes Valjean an outlier in the DC1-DC2 space. The property \textit{Betweenness} is defined as the number of shortest paths going through a vertex \citep{igraph}. Thus Valjean becomes a node in many shortest paths in the character network making him the main outlier.  

\subsection{Classics from Gutenberg}\label{sec:ResWithVis6}
This example is on text analysis of $22$ classics downloaded from the Gutenberg project \citep{gutenberg} and  is similar to the example in \cite{wilkinson2017visualizing}. We consider the novels \textit{Alice in Wonderland}, \textit{Anna Karenina}, \textit{Bleak House}, \textit{Emma}, \textit{Frankenstein}, \textit{Gullivers Travels}, \textit{Jude the Obscure}, \textit{Lord Jim}, \textit{Mansfield Park}, \textit{Middlemarch}, \textit{Moby Dick}, \textit{Northanger Abbey}, \textit{Persuasion}, \textit{Pride and Prejudice}, \textit{Sense and Sensibility}, \textit{Silas Marner}, \textit{Sons and Lovers}, \textit{The Life and Opinions of Tristram Shandy}, \textit{Wizard of Oz}, \textit{Ulysses}, \textit{Vanity Fair} and  \textit{War and Peace} in our analysis. \\

For each novel we obtain the collection of words used. This set of words need to be cleaned before computing any useful features. We use the R package \textit{tidytext} \citep{tidytext} for this task. As the first `cleaning' step, we eliminate the stop words such as \textit{the}, \textit{a}, \textit{an} and \textit{in} from our collection. We also remove numbers from our word collection.  Next we use a stemming procedure to reduce words to their word stem. For example the words \textit{run} and \textit{running} have the same word stems. This process gives us a cleaned set of words for each novel. We compute the tf-idf (term frequency-inverse document frequency) measure on this collection. The tf-idf statistic measures how important a word is to a document when considering a collection of documents.  Consequently we end up with a rectangular dataset of $22$ observations and $39,624$ variables, where each observation is a novel and each variable is the tf-idf statistic of a word. \\

However, $22$ vectors in $\mathbb{R}^{39,624}$ can only span a subspace of $22$ dimensions or less. As such, we compute this subspace and the respective coordinates of the observations using Principal Component Analysis. We use all the principal components so that we're  performing a change of basis and not a projection. As a result we obtain a $22\times 22$ matrix, where each row is an observation corresponding to a novel and each column is a linear combination of $39,624$ tf-idf statistics. The resulting O3 plot using \textit{HDoutliers} and the corresponding DC1-DC2 space is illustrated in Figure~\ref{fig:gutenberg}. Again we see the O3 plot and the DC1-DC2 space agreeing on the most outlying novel \textit{Ulysses} confirming the findings of \cite{wilkinson2017visualizing}. Performing a KNN algorithm yielded \textit{Ulysses} as the observation with the highest KNN distance.  \\

The novel \textit{Ulysses} stands out in the DC1-DC2 space mainly because of its second DC component. However, we do not explore the coefficients of this vector as each coefficient is a linear combination of  $39,624$ variables, making it difficult to gain insights. \\

\begin{figure*}[!t]
	\centering
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex6_1.pdf}
		\label{fig:gutenberg1}
	}%
	\subfloat[][]{
		\includegraphics[scale=0.48]{Ex6_2.pdf}
		\label{fig:gutenberg2}
	}%
	\caption{O3plot of 22 Gutenberg classics in Figure~\ref{fig:gutenberg1} and the first 2 DOBIN components in Figure~\ref{fig:gutenberg2} }
	\label{fig:gutenberg}
\end{figure*}

This completes the visual analysis of datasets. Next we  explore the effect of DOBIN on an extensive data repository. 

% =======================================================================
\section{Results on a data repository}\label{sec:DatRepo}
% =======================================================================
In this section we work with a data repository of more than $12,000$ datasets \citep{datasets}, which was used in our earlier work. Each dataset has labeled outliers and  originally was used for classification purposes. The present datasets are modified from its original form by downsampling minority class observations, converting categorical attributes to numerical values, removing missing data and creating several variants from each classification dataset. More details on dataset preparation can be found in \cite{kandanaarachchi2018normalization}. This collection also includes datasets from \cite{campos2016evaluation}. \\ 

Similar to Section~\ref{sec:synthetic}, for each dataset we compare the results of three outlier methods LOF, KNN and iForest using the area under the ROC curve (AUC), on three different coordinate systems, namely the full set of coordinates, the first half of DOBIN components and the first half of Principal Components. For a dataset with $p$ variables and $N$ observations, if $p > N$, we consider $N/2$ number of DOBIN and PC components. This is because when $p >N$, \,  $N$  vectors can only span an $N$ dimensional space.  \\

We conduct two experiments on this data repository. For the synthetic data experiments conducted in Section~\ref{sec:synthetic}, DOBIN was quite effective in revealing hidden outliers in subspaces. In order to test this intuition rigorously, for each dataset we add $20$ noise variables distributed normally, i.e. $\sim \mathcal{N}(0,1)$.  We compare the results of the three outlier detection methods on the three sets of coordinates on each of these `fattened' datasets. This is our first experiment. As the second experiment, we perform the same computation on the original datasets without any changes. \\

We need to select appropriate tools to present results of $12,000$ datasets across three methods and three coordinate systems. The ability to delve into each dataset and obtain  insights is no longer feasible with a large repository. Furthermore, the datasets are generated from approximately $200$ source datasets. Thus, dataset variants need to be accounted for. \\

In addition, we would like to ascertain whether DOBIN coordinates improve the performance of the three outlier detection methods compared to the other two coordinate systems. Therefore we investigate each outlier detection method separately. For each outlier method, we compare the performance of the three coordinate systems using a Friedman test adjusting for the dataset variants. If the Friedman test is significant, then we perform a Nemenyi test, so that we know the rankings of the coordinate systems.  

\subsection{Fattened datasets}\label{sec:DatRepo1}
$12,647$ datasets


\begin{figure*}[!t]
	\centering
	\subfloat[][]{
		\includegraphics[scale=0.48]{LOF_Nemenyi.pdf}
		\label{fig:LOF_FAT1}
	}%
	\subfloat[][]{
		\includegraphics[scale=0.48]{LOF_Density.pdf}
		\label{fig:LOF_FAT2}
	}%
	\caption{Nemenyi plot of LOF results of fattened datasets in Figure~\ref{fig:LOF_FAT1} and the density plot of the three coordinate systems in Figure~\ref{fig:LOF_FAT2}. }
	\label{fig:LOF_FAT}
\end{figure*}



% =======================================================================
\section{Conclusion}
% =======================================================================
FOR LATER!

% =======================================================================
\footnotesize
\bibliographystyle{agsm} %Choose a bibliograhpic style
\bibliography{Master}
% =======================================================================
\end{document}
